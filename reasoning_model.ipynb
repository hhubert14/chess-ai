{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wg6yIQ1bkrx6"
      },
      "outputs": [],
      "source": [
        "def is_running_in_colab():\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        print(\"Running in Google Colab\")\n",
        "        return True\n",
        "    except ImportError:\n",
        "        print(\"Running in local environment\")\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqSuP92aiQqG",
        "outputId": "2f551483-bf3c-4414-a1c3-28c1c9d8059d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Google Colab\n",
            "Requirement already satisfied: unsloth in /usr/local/lib/python3.11/dist-packages (2025.2.12)\n",
            "Collecting git+https://github.com/unslothai/unsloth.git\n",
            "  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-req-build-2p9d9co3\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-req-build-2p9d9co3\n",
            "  Resolved https://github.com/unslothai/unsloth.git to commit d1d15f1d14f1168837d29b9c08e9b6d63945d469\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: unsloth\n",
            "  Building wheel for unsloth (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unsloth: filename=unsloth-2025.2.12-py3-none-any.whl size=187179 sha256=903d75f0d9bdd709a56f8d4ca8beca601f42384254e2de97c2dbfa94d503635f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-swglkic4/wheels/d1/17/05/850ab10c33284a4763b0595cd8ea9d01fce6e221cac24b3c01\n",
            "Successfully built unsloth\n",
            "Installing collected packages: unsloth\n",
            "  Attempting uninstall: unsloth\n",
            "    Found existing installation: unsloth 2025.2.12\n",
            "    Uninstalling unsloth-2025.2.12:\n",
            "      Successfully uninstalled unsloth-2025.2.12\n",
            "Successfully installed unsloth-2025.2.12\n"
          ]
        }
      ],
      "source": [
        "if is_running_in_colab():\n",
        "    # %%capture\n",
        "    !pip install unsloth\n",
        "    !pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amoB-MCgiQqH",
        "outputId": "8279ccb9-7023-4e64-b99b-b37f0121967b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Google Colab\n"
          ]
        }
      ],
      "source": [
        "if is_running_in_colab():\n",
        "    from google.colab import userdata\n",
        "    HUGGINGFACE_TOKEN = userdata.get(\"HUGGINGFACE_TOKEN\")\n",
        "else:\n",
        "    import os\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv()\n",
        "    HUGGINGFACE_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 760
        },
        "id": "3YtiIQfuiQqH",
        "outputId": "72ba7d36-54c7-4433-c492-c716fc1f0b5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: WARNING `trust_remote_code` is True.\n",
            "Are you certain you want to do remote code execution?\n",
            "==((====))==  Unsloth 2025.2.12: Fast Llama patching. Transformers: 4.48.3.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/accelerate/utils/modeling.py:1536: UserWarning: Current model requires 4194304.0 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-19123749bd4e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mload_in_4bit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m model, tokenizer = FastLanguageModel.from_pretrained(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmex_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/loader.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, use_exact_model_name, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, *args, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         model, tokenizer = dispatch_model.from_pretrained(\n\u001b[0m\u001b[1;32m    293\u001b[0m             \u001b[0mmodel_name\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0mmax_seq_length\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/llama.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, **kwargs)\u001b[0m\n\u001b[1;32m   1772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1773\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfast_inference\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1774\u001b[0;31m             model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m   1775\u001b[0m                 \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1776\u001b[0m                 \u001b[0mdevice_map\u001b[0m              \u001b[0;34m=\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    565\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4187\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhf_quantizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4188\u001b[0;31m                 \u001b[0mhf_quantizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_environment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4190\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mdevice_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/quantizers/quantizer_bnb_4bit.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_map_without_lm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"disk\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_map_without_lm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    104\u001b[0m                     \u001b[0;34m\"Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0;34m\"quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "mex_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
        "    max_seq_length=mex_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        "    trust_remote_code=True,\n",
        "    token=HUGGINGFACE_TOKEN\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fine-tuning"
      ],
      "metadata": {
        "id": "rIAiHVWtHWGt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iThUksBRiQqI",
        "outputId": "d11398ef-3371-49a5-e607-9f33f9955c87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.2.12 patched 48 layers with 48 QKV layers, 48 O layers and 48 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=3407,\n",
        "    use_rslora=False,\n",
        "    loftq_config=None,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9cm-PjdGGkLp"
      },
      "outputs": [],
      "source": [
        "train_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\n",
        "Write a response that appropriately completes the request.\n",
        "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
        "\n",
        "### Instruction:\n",
        "You are a chess expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning.\n",
        "Please answer the following medical question.\n",
        "\n",
        "### Question:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "<think>\n",
        "{}\n",
        "</think>\n",
        "{}\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "3Mn6ZSwbHb2W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "SJMasnADGkLq"
      },
      "outputs": [],
      "source": [
        "prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\n",
        "Write a response that appropriately completes the request.\n",
        "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
        "\n",
        "### Instruction:\n",
        "You are a chess expert with advanced knowledge in strategy, tactics, positional play, and endgame theory.\n",
        "Please analyze the following chess position or question and provide a detailed response.\n",
        "\n",
        "### Question:\n",
        "{}\n",
        "Analyze this chess position by addressing:\n",
        "1. Material and piece placement\n",
        "2. Control of key squares and pawn structure\n",
        "3. Your calculated main line (2-3 moves deep)\n",
        "4. Strategic plans for both sides\n",
        "Then, on a new line, recommend the best move based on this analysis.\n",
        "Write the best move in UCI notation (e.g., e2e4, e7e5, e1g1, e7e8q...)\n",
        "\n",
        "### Response:\n",
        "<think>{}\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\n",
        "Write a response that appropriately completes the request.\n",
        "Before answering, think carefully about the question and construct a step-by-step reasoning process to ensure a logical and accurate response.\n",
        "\n",
        "### Instruction:\n",
        "You are a chess expert with advanced knowledge in strategy, tactics, positional play, and endgame theory.\n",
        "Please analyze the following chess position or question and provide a detailed response.\n",
        "\n",
        "### Question:\n",
        "{}\n",
        "\n",
        "### Analysis:\n",
        "1. **Material and piece placement**:\n",
        "   - Describe the current material balance and the positioning of key pieces.\n",
        "2. **Control of key squares and pawn structure**:\n",
        "   - Identify key squares controlled by each side and how the pawn structure influences the game.\n",
        "3. **Main line calculation (2-3 moves deep)**:\n",
        "   - Provide a calculated variation with reasoning.\n",
        "4. **Strategic plans for both sides**:\n",
        "   - Suggest key plans and possible maneuvers.\n",
        "\n",
        "### Best Move:\n",
        "Based on the above analysis, the best move is:\n",
        "**UCI Notation:** {}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "aXryVzUOMjNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\n",
        "Write a response that appropriately completes the request.\n",
        "Before answering, think carefully about the question and construct a step-by-step reasoning process to ensure a logical and accurate response.\n",
        "\n",
        "### Instruction:\n",
        "K, Q, R, B, N, P are the notations for the white king, queen, rook, bishop, knight, and pawn respectively.\n",
        "The notation for the black pieces are the same but with lowercased letters. A period represents an empty square.\n",
        "Please analyze the following chess position:\n",
        "\n",
        "r . b q k b n r\n",
        "p p . . p p p p\n",
        ". . n p . . . .\n",
        ". B p . . . . .\n",
        ". . . . P . . .\n",
        ". . . . . N . .\n",
        "P P P P . P P P\n",
        "R N B Q K . . R\n",
        "\n",
        "Side to move: White\n",
        "\n",
        "Castling rights: White can castle kingside\n",
        "\n",
        "En passant target square: None\n",
        "\n",
        "### Analysis:\n",
        "1. Material and piece placement:\n",
        "   - Describe the current material balance and the positioning of key pieces.\n",
        "2. Control of key squares and pawn structure:\n",
        "   - Identify key squares controlled by each side and how the pawn structure influences the game.\n",
        "3. Main line calculation (2-3 moves deep):\n",
        "   - Provide a calculated variation with reasoning.\n",
        "4. Strategic plans for both sides:\n",
        "   - Suggest key plans and possible maneuvers.\n",
        "\n",
        "### Best Move:\n",
        "Based on the above analysis, return the best move in UCI notation (e.g., e2e4, e7e5, e1g1, e7e8q...):\n",
        "UCI Notation: [fill in here]\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "9-IOSS8dN3iY"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "hI3yEIdSGkLq",
        "outputId": "cbf9e7ee-0d44-4682-84a7-75cac1c81021",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<｜begin▁of▁sentence｜>Below is an instruction that describes a task, paired with an input that provides further context.\n",
            "Write a response that appropriately completes the request.\n",
            "Before answering, think carefully about the question and construct a step-by-step reasoning process to ensure a logical and accurate response.\n",
            "\n",
            "### Instruction:\n",
            "K, Q, R, B, N, P are the notations for the white king, queen, rook, bishop, knight, and pawn respectively.\n",
            "The notation for the black pieces are the same but with lowercased letters. A period represents an empty square.\n",
            "Please analyze the following chess position:\n",
            "\n",
            "r . b q k b n r\n",
            "p p . . p p p p\n",
            ". . n p . . . .\n",
            ". B p . . . . .\n",
            ". . . . P . . .\n",
            ". . . . . N . .\n",
            "P P P P . P P P\n",
            "R N B Q K . . R\n",
            "\n",
            "Side to move: White\n",
            "\n",
            "Castling rights: White can castle kingside\n",
            "\n",
            "En passant target square: None\n",
            "\n",
            "### Analysis:\n",
            "1. Material and piece placement:\n",
            "   - Describe the current material balance and the positioning of key pieces.\n",
            "2. Control of key squares and pawn structure:\n",
            "   - Identify key squares controlled by each side and how the pawn structure influences the game.\n",
            "3. Main line calculation (2-3 moves deep):\n",
            "   - Provide a calculated variation with reasoning.\n",
            "4. Strategic plans for both sides:\n",
            "   - Suggest key plans and possible maneuvers.\n",
            "\n",
            "### Best Move:\n",
            "Based on the above analysis, return the best move in UCI notation (e.g., e2e4, e7e5, e1g1, e7e8q...):\n",
            "UCI Notation: [fill in here]\n",
            "</think>\n",
            "\n",
            "### Analysis:\n",
            "\n",
            "1. **Material and Piece Placement:**\n",
            "   - **Material Balance:**\n",
            "     - White has a slight material advantage. Key pieces include a queen (Q), rook (R), knight (N), and several pawns (P). Black has a rook (r), bishop (b), knight (n), and pawns (p).\n",
            "     - White's queen is a significant asset, while Black's pieces are less coordinated.\n",
            "   - **Piece Placement:**\n",
            "     - White's queen is on the queenside, supporting a pawn push.\n",
            "     - Black's pieces are spread out, with the rook on the kingside and bishop on the queenside, but they lack coordination.\n",
            "\n",
            "2. **Control of Key Squares and Pawn Structure:**\n",
            "   - **Key Squares:**\n",
            "     - White controls the central squares, particularly e4 and d4, which are crucial for pawn structure and piece activity.\n",
            "     - Black has minimal control over the center, which limits their counterplay.\n",
            "   - **Pawn Structure:**\n",
            "     - White's pawn structure is solid, with a strong center and potential for further expansion.\n",
            "     - Black's pawns are weakly placed, with isolated pawns on the queenside.\n",
            "\n",
            "3. **Main Line Calculation (2-3 Moves Deep):**\n",
            "   - **White's Options:**\n",
            "     - **Option 1:** Qd4, preparing to dominate the center.\n",
            "     - **Option 2:** e4, challenging for the center directly.\n",
            "   - **Black's Responses:**\n",
            "     - To **Qd4**, Black might play Nxe4, but this weakens the center and allows White to gain a strong position.\n",
            "     - To **e4**, Black could try to challenge with ...e5, but this would weaken their center and allow White to maintain control.\n",
            "\n",
            "4. **Strategic Plans for Both Sides:**\n",
            "   - **White's Key Plans:**\n",
            "     - Dominate the center with the queen and pawns.\n",
            "     - Prepare for a kingside attack, supported by castling rights.\n",
            "   - **Black's Key Plans:**\n",
            "     - Attempt to disrupt White's center by challenging pawns.\n",
            "     - Coordinate pieces to counterattack, though this seems difficult given the current position.\n",
            "\n",
            "### Best Move:\n",
            "Based on the analysis, the best move for White is to challenge for the center and gain a strong positional advantage. The move **e4** is recommended as it directly challenges Black's pawn structure and opens lines for piece activity.\n",
            "\n",
            "**UCI Notation:** e4<｜end▁of▁sentence｜>\n"
          ]
        }
      ],
      "source": [
        "question = \"Black to move. Position is 8/8/3k4/R7/8/n2K4/8/8 b - - 18 70. What is the best move?\"\n",
        "\n",
        "FastLanguageModel.for_inference(model)\n",
        "inputs = tokenizer([prompt_style], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids=inputs.input_ids,\n",
        "    attention_mask=inputs.attention_mask,\n",
        "    max_new_tokens=10000,\n",
        "    use_cache=True,\n",
        ")\n",
        "\n",
        "response = tokenizer.batch_decode(outputs)\n",
        "# print(response[0].split(\"### Best Move:\")[1])\n",
        "print(response[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mp4FOrAxGkLq"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}